{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yk0vCfVaKxw5",
    "outputId": "e5e6183c-9e79-4c02-edfe-e48a14ba5cd0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Imports\n",
    "'''\n",
    "\n",
    "# General\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "try:\n",
    "  import wandb\n",
    "except:\n",
    "  ! pip install wandb\n",
    "  import wandb\n",
    "\n",
    "\n",
    "try:\n",
    "  from datasets import Dataset\n",
    "except:\n",
    "  ! pip install datasets\n",
    "  from datasets import Dataset\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Transformers\n",
    "try:\n",
    "    from transformers import (\n",
    "        T5TokenizerFast,\n",
    "        T5Tokenizer,\n",
    "        T5ForConditionalGeneration,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForSeq2Seq,\n",
    "        Seq2SeqTrainer,\n",
    "        Seq2SeqTrainingArguments\n",
    "    )\n",
    "except:\n",
    "    ! pip install transformers\n",
    "    from transformers import (\n",
    "        T5TokenizerFast,\n",
    "        T5Tokenizer,\n",
    "        T5ForConditionalGeneration,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForSeq2Seq,\n",
    "        Seq2SeqTrainer,\n",
    "        Seq2SeqTrainingArguments\n",
    "    )\n",
    "\n",
    "! pip install accelerate\n",
    "! pip install fuzzywuzzy[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6gLbxdLOzxM",
    "outputId": "461d678f-f6a0-435d-c8a2-d3c228afd284"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "from google.colab import drive\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "output_dir = \"/content/drive/MyDrive/t5_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faDp8TdrMUZd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Formats raw email chain and structured tenant profile data for T5 model training or inference.\n",
    "\n",
    "Args:\n",
    "    - data (List[Dict]): A list of dictionaries, where each dictionary contains:\n",
    "        - \"email_chain\": A list of emails (each with a \"body\" key).\n",
    "        - \"tenant_profile\": A nested dictionary with tenant metadata.\n",
    "\n",
    "Returns:\n",
    "    - List[Dict[str, str]]: A list of dictionaries, each containing:\n",
    "        - \"input_text\": A prompt string (email conversation).\n",
    "        - \"target_text\": A semicolon-delimited string encoding the tenant profile fields.\n",
    "\"\"\"\n",
    "def format_for_t5(data: List[Dict]) -> List[Dict[str, str]]:\n",
    "    extracted = []\n",
    "    for item in data:\n",
    "        email_text = \" \".join(email[\"body\"].replace(\"\\n\", \" \") for email in item[\"email_chain\"])\n",
    "        input_text = f\"extract tenant profile: {email_text}\"\n",
    "\n",
    "        profile = item.get(\"tenant_profile\", {})\n",
    "        prefs = profile.get(\"Property Preferences\", {})\n",
    "\n",
    "        fields = {\n",
    "            \"FirstName\": profile.get(\"Tenant Representative Details\", {}).get(\"First Name\", \"\"),\n",
    "            \"LastName\": profile.get(\"Tenant Representative Details\", {}).get(\"Last Name\", \"\"),\n",
    "            \"Email\": profile.get(\"Tenant Representative Details\", {}).get(\"Email\", \"\"),\n",
    "            \"Phone\": profile.get(\"Tenant Representative Details\", {}).get(\"Phone\", \"\"),\n",
    "            \"CompanyName\": profile.get(\"Company Details\", {}).get(\"Company Name\", \"\"),\n",
    "            \"Industry\": profile.get(\"Company Details\", {}).get(\"Industry\", \"\"),\n",
    "            \"CompanySize\": profile.get(\"Company Details\", {}).get(\"Company Size\", \"\"),\n",
    "            \"GrowthStage\": profile.get(\"Company Details\", {}).get(\"Growth Stage\", \"\"),\n",
    "            \"CurrentNeighborhood\": \"; \".join(profile.get(\"Company Details\", {}).get(\"Current Neighborhood\", [])),\n",
    "            \"FirstInteraction\": profile.get(\"First Interaction\", \"\"),\n",
    "            \"LastInteraction\": profile.get(\"Last Interaction\", \"\"),\n",
    "            \"DecisionMakerRole\": profile.get(\"Decision-Maker Role\", \"\"),\n",
    "            \"PropertyType\": prefs.get(\"Property Type\", \"\"),\n",
    "            \"PreferredNeighborhood\": \"; \".join(prefs.get(\"Preferred Neighborhood\", [])),\n",
    "            \"Budget\": prefs.get(\"Estimated or Stated Budget\", \"\"),\n",
    "            \"MustHaves\": \"; \".join(prefs.get(\"Must-Haves\", [])),\n",
    "            \"NiceToHaves\": \"; \".join(prefs.get(\"Nice-to-Haves\", [])),\n",
    "            \"SpaceSize\": prefs.get(\"Space Size\", \"\"),\n",
    "            \"PreferredLeaseTerm\": prefs.get(\"Preferred Lease Term\", \"\"),\n",
    "            \"MovingTerm\": prefs.get(\"Moving Term\", \"\"),\n",
    "            \"min_months\": str(prefs.get(\"Moving Timing\", {}).get(\"min_months\", \"\")),\n",
    "            \"max_months\": str(prefs.get(\"Moving Timing\", {}).get(\"max_months\", \"\")),\n",
    "            \"MovingTimeline\": profile.get(\"Moving Timeline\", \"\"),\n",
    "            \"PainPoint\": \"; \".join(profile.get(\"Pain Points\", [])),\n",
    "            \"UrgencyScore\": str(profile.get(\"Urgency Score\", \"\")),\n",
    "            \"Outcome\": profile.get(\"Outcome\", \"\"),\n",
    "            \"Personality\": profile.get(\"Tenant Personality\", \"\")\n",
    "        }\n",
    "\n",
    "        target_text = \"; \".join(f\"{k}: {v}\" for k, v in fields.items() if v)\n",
    "        extracted.append({\n",
    "            \"input_text\": f\"Extract tenant profile: {email_text}\",\n",
    "            \"target_text\": target_text\n",
    "        })\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWezGXmRMVFJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenizes input-output text pairs for T5 model training or evaluation.\n",
    "\n",
    "Args:\n",
    "    - examples (List[Dict[str, str]]): A list of dictionaries where each dict contains:\n",
    "        - \"input_text\": the prompt text for the model.\n",
    "        - \"target_text\": the expected output for the prompt.\n",
    "    - tokenizer: A HuggingFace tokenizer compatible with T5 (e.g., T5Tokenizer or T5TokenizerFast).\n",
    "    - max_input_length (int, optional): Maximum token length for input sequences. Defaults to 512.\n",
    "    - max_target_length (int, optional): Maximum token length for target sequences. Defaults to 128.\n",
    "\n",
    "Returns:\n",
    "    - Dict[str, torch.Tensor]: A dictionary containing tokenized inputs and labels, including:\n",
    "        - \"input_ids\": token IDs for input_texts\n",
    "        - \"attention_mask\": attention mask for inputs\n",
    "        - \"labels\": token IDs for target_texts\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_t5_examples(examples: List[Dict[str, str]], tokenizer, max_input_length=512, max_target_length=128):\n",
    "    model_inputs = tokenizer(\n",
    "        [ex[\"input_text\"] for ex in examples],\n",
    "        max_length=max_input_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            [ex[\"target_text\"] for ex in examples],\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkXkPhLoMZtO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A custom PyTorch Dataset class for formatting and tokenizing email-based tenant profile data for T5 models.\n",
    "\n",
    "Methods:\n",
    "    __len__(): Returns the number of examples in the dataset.\n",
    "    __getitem__(idx): Returns tokenized input and label tensors for the example at index `idx`.\n",
    "\n",
    "Returns:\n",
    "    - Dict[str, List[int]]: A dictionary containing tokenized fields:\n",
    "        - \"input_ids\": Token IDs for the input prompt\n",
    "        - \"attention_mask\": Mask to ignore padding tokens in the input\n",
    "        - \"labels\": Token IDs for the expected output (target text)\n",
    "\"\"\"\n",
    "\n",
    "class T5EmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "\n",
    "        # Tokenize input text\n",
    "        model_inputs = self.tokenizer(\n",
    "            example[\"input_text\"],\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Tokenize target text as labels\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                example[\"target_text\"],\n",
    "                max_length=self.max_target_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True\n",
    "            )\n",
    "\n",
    "        # Just return input/label token IDs â€” padding masking will be handled by the data collator\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350,
     "referenced_widgets": [
      "8435ff96a09d46868844b1ec1435e3d9",
      "e45a4ba2b8f44b2ea63bfdac1373ff43",
      "301f53cc6574462c966f80add4ef361e",
      "795509ff34dd41c4b9bed5b6d5d66897",
      "574b10fc1e244443a09f27e7c3ac24ca",
      "bc5936ea3a3140c19d82797e9ee2cf5c",
      "8a6701767e6b4f33b876241104495269",
      "e9b03f0e73a145f98dc67661861112a8",
      "5143714accb6451392aa98f57bdbf2aa",
      "c7a8a88613f34b1dbd956f98c0097b79",
      "ac6bf96a0d854b729c181dce5fb6138f",
      "57e4ed252b404b2783619b60eedcfbfe",
      "d989a7f88db847188a865a6357cd757c",
      "aa334d4f856b4d9793d49541095cf0ec",
      "06440fa187e44b068c1cb836dc4fcc02",
      "7b8962559dd94e54af4ab822093be2b1",
      "8b112a45db6e4a989042a9939f8ddfc8",
      "2526e61ad667479c8b80cae07ccf3cc0",
      "5705e94f97d7473797f2439fcca4c24a",
      "cbc851936844406395b402260b127269",
      "d9fb48905250475fa85aeef163d2d275",
      "9348144ed2bc4623b119abec98c76aec",
      "ecbfc5c6d2ac4ffeb596e8ced8d8bb81",
      "4421969e3c054e3b8aeb4ca15c90c4b7",
      "835c1030998548799bd31b47f998443a",
      "7cc8423de83645c1b1318dda252686f0",
      "7641c6da46844a2f9a2d95f5f82c84e8",
      "3664b680e27e4e23bf44c071f90b04b3",
      "5610f21ebf6f40a59e0ef5b86da4d6a3",
      "f9e57f348d3547a19255f29aa8973712",
      "92988735724742acaebb55bbb24adce7",
      "02bd56b5d1d34100aa5978eca119b57e",
      "09a2b730d8da4f7f96b7bcad62b1c9ba",
      "14c273f334734cf482aa3bb76f7cda0f",
      "94aa167ab71c46bebf0a24875bf74004",
      "3d12e56a90e84cf1b7517628b1e63b6b",
      "5b0e5210b47546b3bb161efddc572a49",
      "a3f74c8a71114d3a9dce8137311f7d92",
      "c41834f772d04423b72647dd11650c07",
      "ca53faa4666841b9b35b504a87e4ef4a",
      "486b58f7194d43ebb2348e0c86e24c2a",
      "f8e8fba05e9340d2b95f70203bc42580",
      "69d47cd3c94746d38e7358877c84c466",
      "0ab0adda64cf406ba57a96d76676c711",
      "b3ba120442714804a9585a588a98a9fe",
      "ca1df822a3ad48b9babcf2e9e02d2137",
      "9ed912850414428abf48fc22c3f39f2a",
      "fb3ea40f8f5c40439cda80e7863124b8",
      "0b7ce4ddd833442f9fd8b9967690ac1c",
      "2a31b9df56b446078503aefd7f34e630",
      "5f067e8f03a94bf18ba28d7dd17ca73f",
      "5081af2039cd4cc499038479ab460b73",
      "0f4fbf68d6054a32b61fa7fcaa58c002",
      "440fec0e13de4cdea8868449378d77c6",
      "3a8ad99c27fd44b2962da09f27d2968d"
     ]
    },
    "id": "MEcyoCLHOm_B",
    "outputId": "09182a02-1203-4ff4-81d4-bfef89914e8c"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Initialize Tokenizer, Model and Data Collator\n",
    "'''\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_length=1024, truncation=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Esk59nTmMeMf",
    "outputId": "f38a8738-f4c0-4f75-bdcf-4e2623cbfefa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read and Process each file\n",
    "'''\n",
    "\n",
    "with open(\"train.json\") as f:\n",
    "  train_data = json.load(f)\n",
    "with open(\"val.json\") as f:\n",
    "  val_data = json.load(f)\n",
    "with open(\"test.json\") as f:\n",
    "  test_data = json.load(f)\n",
    "\n",
    "'''\n",
    "For running with github repo.:\n",
    "with open(\"../3. Data Split/train.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"../3. Data Split/val.json\") as f:\n",
    "    val_data = json.load(f)\n",
    "with open(\"../3. Data Split/test.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "'''\n",
    "\n",
    "train = format_for_t5(train_data)\n",
    "print(train[0])\n",
    "val = format_for_t5(val_data)\n",
    "test= format_for_t5(test_data)\n",
    "\n",
    "train_dataset = T5EmailDataset(train, tokenizer)\n",
    "val_dataset = T5EmailDataset(val, tokenizer)\n",
    "test_dataset = T5EmailDataset(test, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bBIOGagym32X"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse a T5 output string into a dictionary of field-value pairs.\n",
    "\n",
    "Args:\n",
    "    text (str): A semicolon-separated key-value string like 'Budget: $45k/month; SpaceSize: 3,500 sqft'\n",
    "Returns:\n",
    "    dict: Parsed dictionary of extracted values\n",
    "\"\"\"\n",
    "def parse_t5_output(text):\n",
    "\n",
    "    entity_dict = {}\n",
    "    for pair in text.split(\";\"):\n",
    "        if \":\" in pair:\n",
    "            key, value = pair.split(\":\", 1)\n",
    "            entity_dict[key.strip()] = value.strip()\n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSNmFAmUgu47"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluates field-level precision, recall, and F1 score for T5 model outputs in structured tenant profile extraction.\n",
    "\n",
    "Args:\n",
    "    - decoded_preds (List[str]): List of decoded prediction strings from the T5 model.\n",
    "    - decoded_labels (List[str]): List of decoded ground truth target strings.\n",
    "    - fields (List[str]): List of field names (keys) to evaluate (e.g., \"Budget\", \"CompanyName\", etc.).\n",
    "\n",
    "Returns:\n",
    "        - Precision: TP / (TP + FP)\n",
    "        - Recall: TP / (TP + FN)\n",
    "        - F1-Score: Harmonic mean of precision and recall\n",
    "        - Support: Number of non-empty ground-truth entries for that field\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_t5_structured_fieldwise(decoded_preds, decoded_labels, fields):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    field_stats = {field: {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"support\": 0} for field in fields}\n",
    "\n",
    "    for pred_str, label_str in zip(decoded_preds, decoded_labels):\n",
    "        pred_dict = parse_t5_output(pred_str)\n",
    "        label_dict = parse_t5_output(label_str)\n",
    "\n",
    "        for field in fields:\n",
    "            y_true_val = label_dict.get(field, \"\").strip()\n",
    "            y_pred_val = pred_dict.get(field, \"\").strip()\n",
    "\n",
    "            if y_true_val:\n",
    "                field_stats[field][\"support\"] += 1\n",
    "\n",
    "                if y_pred_val == y_true_val:\n",
    "                    field_stats[field][\"tp\"] += 1\n",
    "                elif y_pred_val:\n",
    "                    field_stats[field][\"fp\"] += 1\n",
    "                    field_stats[field][\"fn\"] += 1\n",
    "                else:\n",
    "                    field_stats[field][\"fn\"] += 1\n",
    "            elif y_pred_val:\n",
    "                field_stats[field][\"fp\"] += 1\n",
    "    results = {}\n",
    "    macro_p = []\n",
    "    macro_r = []\n",
    "    macro_f1 = []\n",
    "    # Compute and display metrics\n",
    "    print(f\"{'Field':<25} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}\")\n",
    "    for field, stats in field_stats.items():\n",
    "        tp = stats[\"tp\"]\n",
    "        fp = stats[\"fp\"]\n",
    "        fn = stats[\"fn\"]\n",
    "        support = stats[\"support\"]\n",
    "        precision = tp / (tp + fp) if (tp + fp) else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "        print(f\"{field:<25} {precision:10.2f} {recall:10.2f} {f1:10.2f} {support:10}\")\n",
    "        results[f\"{field}_precision\"] = round(precision, 4)\n",
    "        results[f\"{field}_recall\"] = round(recall, 4)\n",
    "        results[f\"{field}_f1\"] = round(f1, 4)\n",
    "        macro_p.append(precision)\n",
    "        macro_r.append(recall)\n",
    "        macro_f1.append(f1)\n",
    "\n",
    "    if macro_p:\n",
    "        macro_precision = round(np.mean(macro_p), 4)\n",
    "        macro_recall = round(np.mean(macro_r), 4)\n",
    "        macro_f1 = round(np.mean(macro_f1), 4)\n",
    "    else:\n",
    "        macro_precision = macro_recall = macro_f1 = 0.0\n",
    "\n",
    "    results[\"macro_precision\"] = macro_precision\n",
    "    results[\"macro_recall\"] = macro_recall\n",
    "    results[\"macro_f1\"] = macro_f1\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWLnX3FE4Tno"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used with Hugging Face Trainer to decode predictions before metrics.\n",
    "Selects the token with the highest logit for each position.\n",
    "\"\"\"\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "\n",
    "    # Take argmax across vocab dimension\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lX-w4GfRe9wN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts and returns a sorted list of unique field/entity keys from structured T5 prediction and label strings.\n",
    "\n",
    "Args:\n",
    "    - decoded_preds (List[str]): List of predicted output strings from the T5 model.\n",
    "    - decoded_labels (List[str]): List of corresponding target label strings.\n",
    "\n",
    "Returns:\n",
    "    - List[str]: Sorted list of unique keys extracted from the ground truth label strings.\n",
    "\n",
    "\"\"\"\n",
    "def extract_entity_keys(decoded_preds, decoded_labels):\n",
    "    raw_keys_preds = []\n",
    "    raw_keys_labels = []\n",
    "\n",
    "    for entry in decoded_preds:\n",
    "        segments = re.findall(r'(\\w[\\w\\s]*?):', entry)\n",
    "        raw_keys_preds.extend([seg.strip().lower() for seg in segments])\n",
    "\n",
    "    for entry in decoded_labels:\n",
    "        segments = re.findall(r'(\\w[\\w\\s]*?):', entry)\n",
    "        raw_keys_labels.extend([seg.strip().lower() for seg in segments])\n",
    "\n",
    "\n",
    "    key_counts = Counter(raw_keys_preds + raw_keys_labels)\n",
    "\n",
    "    print(f\"Preds: {sorted(Counter(raw_keys_preds))}\")\n",
    "    print(f\"Labels: {sorted(Counter(raw_keys_labels))}\")\n",
    "\n",
    "    return sorted(Counter(raw_keys_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoDSd3asXvVr"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function decodes model predictions and labels, extracts structured key-value pairs, compares\n",
    "predicted and ground truth values field-wise, and calculates precision, recall, and F1-score for each\n",
    "field as well as macro-averaged metrics across all fields.\n",
    "\n",
    "Args:\n",
    "    - eval_preds (Tuple[np.ndarray, np.ndarray]):\n",
    "        A tuple where the first element is the raw model predictions (logits or token IDs),\n",
    "        and the second element is the label IDs (with -100 representing masked tokens).\n",
    "\n",
    "Returns:\n",
    "    - Tuple[Dict[str, float], List[str]]:\n",
    "        - A dictionary mapping each field to its precision, recall, and F1-score, including macro-averages.\n",
    "        - A list of decoded prediction strings.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [str(p).strip().lower() for p in decoded_preds]\n",
    "    decoded_labels = [str(l).strip().lower() for l in decoded_labels]\n",
    "    print(f\"Preds: {decoded_preds}\")\n",
    "    print(f\"Labels: {decoded_labels}\")\n",
    "\n",
    "    all_keys = extract_entity_keys(decoded_preds, decoded_labels)\n",
    "\n",
    "    results = evaluate_t5_structured_fieldwise(decoded_preds, decoded_labels, all_keys)\n",
    "\n",
    "    return results, decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "NReOTG543F9D",
    "outputId": "9e9c3a9d-5c0f-430c-ff79-98269acd5783"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define sweep configurations and set up sweep agent\n",
    "'''\n",
    "\n",
    "sweep_config_t5 = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'eval_exact_match',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {'min': 1e-5, 'max': 5e-4},\n",
    "        'per_device_train_batch_size': {'values': [4, 8, 16]},\n",
    "        'num_train_epochs': {'values': [3, 5, 6]},\n",
    "        'label_smoothing_factor': {'values': [0.0, 0.1]},\n",
    "        'warmup_ratio': {'values': [0.0, 0.1]},\n",
    "        'seed': {'values': [42, 123, 2025]}\n",
    "    }\n",
    "}\n",
    "sweep_id_t5 = wandb.sweep(sweep_config_t5, project='t5_final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxT9EH913HOv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains and evaluates a T5 model for structured text-to-text tenant profile extraction.\n",
    "\n",
    "This function initializes a W&B run, configures training hyperparameters using Hugging Face's\n",
    "`TrainingArguments`, and trains the T5 model on the provided training and validation datasets.\n",
    "After training, it evaluates the model on a test set, computes structured field-wise metrics\n",
    "(precision, recall, F1), logs them to W&B, and saves the parsed predictions as a JSON file.\n",
    "\n",
    "Requires global variables:\n",
    "    - model (PreTrainedModel): The initialized T5 model.\n",
    "    - tokenizer (PreTrainedTokenizer): Tokenizer for the T5 model.\n",
    "    - train_dataset, val_dataset, test_dataset: Hugging Face Datasets or compatible PyTorch datasets.\n",
    "    - preprocess_logits_for_metrics, data_collator: Utility functions for training.\n",
    "    - output_dir (str): Path to save final JSON output (should exist or be created before calling).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def train_t5():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./t5_output\",\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            label_smoothing_factor=config.label_smoothing_factor,\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\",\n",
    "            report_to=\"wandb\",\n",
    "            seed=config.seed,\n",
    "            fp16=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            trainer.train()\n",
    "            outputs = trainer.predict(test_dataset)\n",
    "            metrics = compute_metrics((outputs.predictions[0], outputs.label_ids))\n",
    "            print(metrics)\n",
    "            wandb.log({**{f\"test_{k}\": float(v) for k, v in metrics[0].items()}})\n",
    "\n",
    "            def parse_profile_string(profile):\n",
    "                # Use regex to find key-value pairs\n",
    "                pattern = r'(\\w+):\\s*([^;]+)'\n",
    "                matches = re.findall(pattern, profile)\n",
    "                return {key.lower(): value.strip() for key, value in matches}\n",
    "\n",
    "\n",
    "            # Parse each string into a dictionary\n",
    "            parsed_data = [parse_profile_string(profile) for profile in metrics[1]]\n",
    "            print(f\"Parsed: {parsed_data}\")\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(parsed_data)\n",
    "\n",
    "            # Display the DataFrame (optional)\n",
    "            print(df.head())\n",
    "\n",
    "            # Send to wandb\n",
    "            wandb_table = wandb.Table(dataframe=df)\n",
    "            wandb.log({f\"{run.name} Predictions\": wandb_table})\n",
    "\n",
    "            # Save all predictions to Google Drive\n",
    "            pred_path = os.path.join(output_dir, f\"{wandb.run.name}_predictions.json\")\n",
    "            df.to_json(pred_path, orient=\"records\", indent=2)\n",
    "            print(f\"Predictions saved to: {pred_path}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed: {e}\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "evJzBDLJ3HdE",
    "outputId": "ffb51e3c-66cc-429e-8990-9f1d3c4084ff"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Run sweep agent\n",
    "'''\n",
    "wandb.agent(sweep_id_t5, function=train_t5, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDS3QuCxI4Pr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl2En2Qmr3hj"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Predict and Evaluate results on T5 model without training\n",
    "'''\n",
    "\n",
    "def run_base_t5():\n",
    "    print(\"Running base T5 model without training...\")\n",
    "\n",
    "    with wandb.init(project=\"t5_final\", name=\"t5_base\", reinit=True):\n",
    "        try:\n",
    "            input_texts = [ex[\"input_text\"] for ex in test_examples]\n",
    "            inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=128)\n",
    "            print(f\"outputs: {outputs}\")\n",
    "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            decoded_preds = [p.strip().lower() for p in decoded_preds]\n",
    "            decoded_labels = [ex[\"target_text\"].strip().lower() for ex in test_examples]\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = compute_metrics((outputs, tokenizer(decoded_labels, return_tensors=\"np\", padding=True, truncation=True, max_length=128)[\"input_ids\"]))\n",
    "            wandb.log({f\"test_{k}\": v.item() if hasattr(v, \"item\") else float(v) for k, v in metrics[0].items()})\n",
    "            print(metrics)\n",
    "            def parse_profile_string(profile):\n",
    "                # Use regex to find key-value pairs\n",
    "                pattern = r'(\\w+):\\s*([^;]+)'\n",
    "                matches = re.findall(pattern, profile)\n",
    "                return {key.lower(): value.strip() for key, value in matches}\n",
    "\n",
    "\n",
    "            # Parse each string into a dictionary\n",
    "            parsed_data = [parse_profile_string(profile) for profile in decoded_preds]\n",
    "            print(f\"Parsed: {parsed_data}\")\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(parsed_data)\n",
    "\n",
    "            # Display the DataFrame (optional)\n",
    "            print(df.head())\n",
    "\n",
    "            # Save all predictions to Google Drive\n",
    "            pred_path = os.path.join(output_dir, f\"base_predictions.json\")\n",
    "            df.to_json(pred_path, orient=\"records\", indent=2)\n",
    "            print(f\"Predictions saved to: {pred_path}\")\n",
    "\n",
    "            # Send to wandb\n",
    "            wandb_table = wandb.Table(dataframe=df)\n",
    "            wandb.log({\"Base Predictions\": wandb_table})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Base inference failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vxNI_5tevC9P",
    "outputId": "04229e28-c635-4c85-e42d-451c5cffd2d4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Run base model\n",
    "'''\n",
    "run_base_t5()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
