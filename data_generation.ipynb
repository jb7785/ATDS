{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "wt39_L_FHYwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "69BJi5oeGxKz"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import hashlib\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Configuration"
      ],
      "metadata": {
        "id": "IM_LtmTMHecY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check OpenAI version for compatibility\n",
        "OPENAI_VERSION = openai.__version__\n",
        "from openai import RateLimitError, AuthenticationError, OpenAI\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Set the model to use\n",
        "model = \"gpt-4.1-nano\""
      ],
      "metadata": {
        "id": "1v_7iBY_HgEH"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EmailChainGenerator"
      ],
      "metadata": {
        "id": "z0kRDo6iHpLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailChainGenerator:\n",
        "    \"\"\"\n",
        "    A generator for producing synthetic email threads between commercial real estate brokers\n",
        "    and prospective tenants in Manhattan with enhanced uniqueness and diversity.\n",
        "    \"\"\"\n",
        "    def __init__(self, prompt, num_chains=1, model=\"gpt-4.1-nano\", dry_run=False):\n",
        "        \"\"\"\n",
        "        Initializes the EmailChainGenerator.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The prompt to send to the OpenAI model.\n",
        "            num_chains (int): Number of email chains to generate.\n",
        "            model (str): The OpenAI model to use.\n",
        "            dry_run (bool): If True, use a local JSON file instead of calling the API.\n",
        "        \"\"\"\n",
        "        self.prompt = prompt\n",
        "        self.num_chains = num_chains\n",
        "        self.dataset = []\n",
        "        self.dry_run = dry_run\n",
        "        self.model = model\n",
        "        self.client = OpenAI(api_key=openai.api_key)\n",
        "        self.used_names = self.load_used_names()\n",
        "        self.used_contexts = self.load_used_contexts()\n",
        "        self.industry_variations = [\n",
        "            \"Tech\", \"Finance\", \"Legal\", \"Healthcare\", \"Nonprofit\", \"Retail\", \"Creative\", \"Education\", \"Hospitality\"\n",
        "        ]\n",
        "        self.neighborhoods = [\n",
        "            \"Flatiron\", \"Tribeca\", \"Hudson Yards\", \"East Village\", \"Grand Central\", \"Chelsea\", \"SoHo\", \"Midtown East\", \"Financial District\"\n",
        "        ]\n",
        "        self.personalities = [\n",
        "            \"Enthusiastic\", \"Skeptical\", \"Detail-obsessed\", \"Price-sensitive\", \"Decisive\", \"Hesitant\"\n",
        "        ]\n",
        "\n",
        "    def load_used_names(self):\n",
        "        \"\"\"Load used names and companies from a file to ensure uniqueness.\"\"\"\n",
        "        try:\n",
        "            with open(\"used_names.json\", \"r\") as f:\n",
        "                return json.load(f)\n",
        "        except (FileNotFoundError, json.JSONDecodeError):\n",
        "            return {\"names\": [], \"companies\": [], \"emails\": []}\n",
        "\n",
        "    def save_used_names(self):\n",
        "        \"\"\"Save used names, companies, and emails to a file atomically.\"\"\"\n",
        "        try:\n",
        "            temp_file = \"used_names_temp.json\"\n",
        "            with open(temp_file, \"w\") as f:\n",
        "                json.dump(self.used_names, f, indent=2)\n",
        "            os.replace(temp_file, \"used_names.json\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving used names: {e}\")\n",
        "\n",
        "    def load_used_contexts(self):\n",
        "        \"\"\"Load used context hashes to ensure unique situations.\"\"\"\n",
        "        try:\n",
        "            with open(\"used_contexts.json\", \"r\") as f:\n",
        "                return json.load(f)\n",
        "        except (FileNotFoundError, json.JSONDecodeError):\n",
        "            return []\n",
        "\n",
        "    def save_used_contexts(self):\n",
        "        \"\"\"Save used context hashes to a file atomically.\"\"\"\n",
        "        try:\n",
        "            temp_file = \"used_contexts_temp.json\"\n",
        "            with open(temp_file, \"w\") as f:\n",
        "                json.dump(self.used_contexts, f, indent=2)\n",
        "            os.replace(temp_file, \"used_contexts.json\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving used contexts: {e}\")\n",
        "\n",
        "    def create_context_hash(self, tenant_profile):\n",
        "        \"\"\"Create a hash of key tenant profile fields to ensure unique contexts.\"\"\"\n",
        "        fields = [\n",
        "            tenant_profile[\"Company Details\"][\"Industry\"],\n",
        "            tenant_profile[\"Company Details\"][\"Company Size\"],\n",
        "            tenant_profile[\"Company Details\"][\"Growth Stage\"],\n",
        "            tenant_profile[\"Property Preferences\"][\"Property Type\"],\n",
        "            tenant_profile[\"Property Preferences\"][\"Preferred Neighborhood\"],\n",
        "            tenant_profile[\"Property Preferences\"][\"Estimated or Stated Budget\"],\n",
        "            tenant_profile[\"Property Preferences\"][\"Space Size\"],\n",
        "            tenant_profile[\"Property Preferences\"][\"Preferred Lease Term\"],\n",
        "            tenant_profile[\"Moving Timeline\"],\n",
        "            tenant_profile[\"Outcome\"],\n",
        "            \",\".join(sorted(tenant_profile[\"Pain Points\"])),\n",
        "            \",\".join(sorted(tenant_profile[\"Property Preferences\"][\"Must-Haves\"])),\n",
        "            \",\".join(sorted(tenant_profile[\"Property Preferences\"][\"Nice-to-Haves\"])),\n",
        "            tenant_profile[\"Decision-Maker Role\"],\n",
        "            tenant_profile.get(\"Tenant Personality\", \"Unknown\")\n",
        "        ]\n",
        "        context_str = \"|\".join(str(field) for field in fields)\n",
        "        return hashlib.sha256(context_str.encode()).hexdigest()\n",
        "\n",
        "    def enhance_prompt(self, retry_count=0):\n",
        "        \"\"\"Enhance the prompt to enforce uniqueness and diversity.\"\"\"\n",
        "        used_names_str = \", \".join(self.used_names[\"names\"]) if self.used_names[\"names\"] else \"none\"\n",
        "        used_companies_str = \", \".join(self.used_names[\"companies\"]) if self.used_names[\"companies\"] else \"none\"\n",
        "        used_emails_str = \", \".join(self.used_names[\"emails\"]) if self.used_names[\"emails\"] else \"none\"\n",
        "        context_guidance = (\n",
        "            f\"Generate a unique tenant profile with a distinct combination of:\\n\"\n",
        "            f\"- Industry: Choose from {', '.join(self.industry_variations)} or a novel industry.\\n\"\n",
        "            f\"- Neighborhood: Choose from {', '.join(self.neighborhoods)} or a new Manhattan area.\\n\"\n",
        "            f\"- Personality: Choose from {', '.join(self.personalities)}.\\n\"\n",
        "            f\"- Unique pain points, budget, timeline, and outcome.\\n\"\n",
        "            f\"Avoid any similarity to previous names, companies, emails, or contexts. \"\n",
        "            f\"Create a new company name and representative name that do not resemble: {used_names_str}, {used_companies_str}, {used_emails_str}. \"\n",
        "            f\"Ensure the email chain reflects realistic Manhattan CRE dynamics, with varied tenant personalities and broker tactics.\"\n",
        "        )\n",
        "        retry_instruction = (\n",
        "            f\"Generate a completely different context from previous attempts. \"\n",
        "            f\"This is attempt {retry_count + 1}. Ensure valid JSON output with no syntax errors.\"\n",
        "        ) if retry_count > 0 else \"\"\n",
        "        enhanced_prompt = (\n",
        "            self.prompt.replace(\"[DYNAMIC_LIST_NAMES]\", used_names_str)\n",
        "            .replace(\"[DYNAMIC_LIST_COMPANIES]\", used_companies_str)\n",
        "            .replace(\"[DYNAMIC_LIST_CONTEXTS]\", \"none\")  # Contexts are handled via hash\n",
        "            + f\"\\n\\n{context_guidance}\\n\\n{retry_instruction}\"\n",
        "        )\n",
        "        return enhanced_prompt\n",
        "\n",
        "    def clean_response_text(self, text):\n",
        "        \"\"\"Clean API response to extract valid JSON.\"\"\"\n",
        "        text = re.sub(r'^```json\\s*|\\\\s*```$', '', text, flags=re.MULTILINE)\n",
        "        text = text.strip()\n",
        "        if not text.startswith('{') or not text.endswith('}'):\n",
        "            return None\n",
        "        return text\n",
        "\n",
        "    def generate_email_chain(self):\n",
        "        \"\"\"\n",
        "        Generates a single email chain and corresponding tenant profile.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (email_chain, tenant_profile) or (None, None) on error.\n",
        "        \"\"\"\n",
        "        if self.dry_run:\n",
        "            try:\n",
        "                with open(\"dry_run.txt\", \"r\") as f:\n",
        "                    dry_run_txt = f.read()\n",
        "                    parsed = json.loads(dry_run_txt)\n",
        "                    email_chain = parsed.get(\"email_chain\")\n",
        "                    tenant_profile = parsed.get(\"tenant_profile\")\n",
        "                    return email_chain, tenant_profile\n",
        "            except FileNotFoundError:\n",
        "                print(\"Error: dry_run.txt file not found.\")\n",
        "                return None, None\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing dry_run.txt as JSON: {e}\")\n",
        "                return None, None\n",
        "\n",
        "        max_retries = 10  # Increase retries for better uniqueness\n",
        "        for retry in range(max_retries):\n",
        "            try:\n",
        "                prompt = self.enhance_prompt(retry)\n",
        "                response = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are an email thread generator for real estate. Output only valid JSON with no additional text.\"},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    temperature=0.8,  # Increase temperature for more diversity\n",
        "                    max_tokens=4000\n",
        "                )\n",
        "                response_text = response.choices[0].message.content\n",
        "                cleaned_text = self.clean_response_text(response_text)\n",
        "                if not cleaned_text:\n",
        "                    print(f\"Error: Invalid JSON structure in API response on attempt {retry + 1}.\")\n",
        "                    continue\n",
        "                parsed = json.loads(cleaned_text)\n",
        "                email_chain = parsed[\"email_chain\"]\n",
        "                tenant_profile = parsed[\"tenant_profile\"]\n",
        "\n",
        "                # Validate email chain structure\n",
        "                if not isinstance(email_chain, list) or not all(\n",
        "                    isinstance(email, dict) and all(key in email for key in [\"timestamp\", \"from\", \"to\", \"subject\", \"body\"])\n",
        "                    for email in email_chain\n",
        "                ):\n",
        "                    print(f\"Error: Invalid email chain structure or missing email bodies on attempt {retry + 1}.\")\n",
        "                    continue\n",
        "\n",
        "                # Validate name, company, and email\n",
        "                first_name = tenant_profile[\"Tenant Representative Details\"][\"First Name\"]\n",
        "                last_name = tenant_profile[\"Tenant Representative Details\"][\"Last Name\"]\n",
        "                company = tenant_profile[\"Company Details\"][\"Company Name\"]\n",
        "                email = tenant_profile[\"Tenant Representative Details\"][\"Email\"]\n",
        "                full_name = f\"{first_name} {last_name}\"\n",
        "                if (full_name in self.used_names[\"names\"] or\n",
        "                    company in self.used_names[\"companies\"] or\n",
        "                    email in self.used_names[\"emails\"]):\n",
        "                    print(f\"Error: Duplicate name '{full_name}', company '{company}', or email '{email}' detected on attempt {retry + 1}.\")\n",
        "                    continue\n",
        "\n",
        "                # Validate context\n",
        "                context_hash = self.create_context_hash(tenant_profile)\n",
        "                if context_hash in self.used_contexts:\n",
        "                    print(f\"Error: Duplicate context detected on attempt {retry + 1}.\")\n",
        "                    continue\n",
        "\n",
        "                # Add to used names, companies, emails, and contexts\n",
        "                self.used_names[\"names\"].append(full_name)\n",
        "                self.used_names[\"companies\"].append(company)\n",
        "                self.used_names[\"emails\"].append(email)\n",
        "                self.used_contexts.append(context_hash)\n",
        "                self.save_used_names()\n",
        "                self.save_used_contexts()\n",
        "\n",
        "                return email_chain, tenant_profile\n",
        "\n",
        "            except (RateLimitError, AuthenticationError) as e:\n",
        "                print(f\"OpenAI API error: {e}\")\n",
        "                return None, None\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error parsing API response as JSON: {e}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating email chain: {e}\")\n",
        "                return None, None\n",
        "\n",
        "        print(f\"Failed to generate unique chain after {max_retries} attempts.\")\n",
        "        return None, None\n",
        "\n",
        "    def validate_data(self, email_chain, tenant_profile):\n",
        "        \"\"\"\n",
        "        Validates the email chain and tenant profile.\n",
        "\n",
        "        Args:\n",
        "            email_chain: List of email dictionaries.\n",
        "            tenant_profile: The tenant profile.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if valid, False otherwise.\n",
        "        \"\"\"\n",
        "        required_profile_keys = [\n",
        "            \"Tenant Representative Details\", \"Company Details\", \"Property Preferences\",\n",
        "            \"First Interaction\", \"Last Interaction\", \"Moving Timeline\", \"Pain Points\",\n",
        "            \"Urgency Score\", \"Outcome\", \"Tenant Personality\"\n",
        "        ]\n",
        "        if not email_chain or not tenant_profile:\n",
        "            return False\n",
        "        if not all(key in tenant_profile for key in required_profile_keys):\n",
        "            print(\"Warning: Missing required keys in tenant profile.\")\n",
        "            return False\n",
        "        if not (2 <= len(email_chain) <= 8):\n",
        "            print(\"Warning: Email chain length must be 2â€“8 emails.\")\n",
        "            return False\n",
        "        for email in email_chain:\n",
        "            if not email.get(\"body\"):\n",
        "                print(\"Warning: Email missing body.\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def save_as_json(self, filename=\"synthetic_email_data.json\"):\n",
        "        \"\"\"\n",
        "        Appends the dataset to an existing JSON file or creates a new one.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The file path where the dataset should be saved.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            try:\n",
        "                with open(filename, \"r\") as f:\n",
        "                    existing_data = json.load(f)\n",
        "            except (FileNotFoundError, json.JSONDecodeError):\n",
        "                existing_data = []\n",
        "\n",
        "            existing_data.extend(self.dataset)\n",
        "\n",
        "            with open(filename, \"w\") as f:\n",
        "                json.dump(existing_data, f, indent=2)\n",
        "            print(f\"Appended {len(self.dataset)} chains to {filename}. Total chains: {len(existing_data)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving data to JSON: {e}\")"
      ],
      "metadata": {
        "id": "NLn88FZv2hIV"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iterations"
      ],
      "metadata": {
        "id": "mzkuBGJRH535"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_prompt():\n",
        "    \"\"\"Load prompt from prompt.txt.\"\"\"\n",
        "    try:\n",
        "        with open(\"prompt.txt\", \"r\") as f:\n",
        "            prompt = f.read()\n",
        "        return prompt\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: prompt.txt not found.\")\n",
        "        raise\n",
        "\n",
        "# Load prompt\n",
        "prompt = load_prompt()\n",
        "print(\"Prompt successfully loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4cPlc7eHp6F",
        "outputId": "45981a89-d5c8-43c0-e3c0-e864402817de"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt successfully loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure generation\n",
        "total_chains = 200  # Target number of chains\n",
        "num_chains_per_batch = 1  # Generate one chain at a time\n",
        "output_file = \"synthetic_email_data.json\"\n",
        "\n",
        "# Load existing dataset to check current chain count\n",
        "try:\n",
        "    with open(output_file, \"r\") as f:\n",
        "        existing_data = json.load(f)\n",
        "    current_chains = len(existing_data)\n",
        "except (FileNotFoundError, json.JSONDecodeError):\n",
        "    existing_data = []\n",
        "    current_chains = 0  # Resume from chain 13\n",
        "\n",
        "chains_to_generate = max(0, total_chains - current_chains)\n",
        "print(f\"Current chains: {current_chains}. Need to generate: {chains_to_generate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8txbeR_PLs3",
        "outputId": "fb6243f9-966d-454e-d9be-b93f25bbe48b"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current chains: 200. Need to generate: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate chains one at a time\n",
        "for i in range(chains_to_generate):\n",
        "    print(f\"Generating chain {current_chains + i + 1}/{total_chains}...\")\n",
        "    generator = EmailChainGenerator(\n",
        "        prompt=prompt,\n",
        "        num_chains=num_chains_per_batch,\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        dry_run=False\n",
        "    )\n",
        "    # Generate one chain\n",
        "    email_chain, tenant_profile = generator.generate_email_chain()\n",
        "\n",
        "    if email_chain is not None and generator.validate_data(email_chain, tenant_profile):\n",
        "        generator.dataset.append({\n",
        "            \"email_chain\": email_chain,\n",
        "            \"tenant_profile\": tenant_profile\n",
        "        })\n",
        "        print(f\"Successfully added chain {current_chains + i + 1}\")\n",
        "        # Append immediately to JSON\n",
        "        generator.save_as_json(output_file)\n",
        "    else:\n",
        "        print(f\"Failed to add chain {current_chains + i + 1}\")\n",
        "\n",
        "print(f\"Generation complete. Total chains in {output_file}: {current_chains + len(generator.dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0BTddGQroqe",
        "outputId": "af35230a-1c59-4789-c531-9f04f21bf1f9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation complete. Total chains in synthetic_email_data.json: 200\n"
          ]
        }
      ]
    }
  ]
}