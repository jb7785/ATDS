{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiZ_zy00aGeS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    \"\"\"Load the synthetic email dataset from a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\")\n",
        "        return []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON format in {file_path}.\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "YtlN_ZsAa6GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset(data, file_path):\n",
        "    \"\"\"Save dataset to a JSON file.\"\"\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    print(f\"Saved dataset to {file_path}\")"
      ],
      "metadata": {
        "id": "cUBS0Ofda7Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(input_file, output_dir, train_size=140, val_size=30, seed=42):\n",
        "    \"\"\"\n",
        "    Split dataset into train, validation, and test sets with exact sizes.\n",
        "    Args:\n",
        "        input_file (str): Path to synthetic_email_data.json\n",
        "        output_dir (str): Directory to save split files\n",
        "        train_size (int): Number of chains for training set (default: 140)\n",
        "        val_size (int): Number of chains for validation set (default: 30)\n",
        "        seed (int): Random seed for reproducibility\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    data = load_dataset(input_file)\n",
        "    if not data:\n",
        "        return\n",
        "\n",
        "    # Verify total size\n",
        "    total_chains = len(data)\n",
        "    test_size = total_chains - train_size - val_size\n",
        "    if total_chains < train_size + val_size:\n",
        "        print(f\"Error: Dataset has {total_chains} chains, but requested {train_size} train + {val_size} val.\")\n",
        "        return\n",
        "    if test_size <= 0:\n",
        "        print(f\"Error: Invalid split sizes (train: {train_size}, val: {val_size}, test: {test_size}).\")\n",
        "        return\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Shuffle data\n",
        "    shuffled_data = data.copy()\n",
        "    random.shuffle(shuffled_data)\n",
        "\n",
        "    # Split data into exact sizes\n",
        "    train_data = shuffled_data[:train_size]\n",
        "    val_data = shuffled_data[train_size:train_size + val_size]\n",
        "    test_data = shuffled_data[train_size + val_size:]\n",
        "\n",
        "    # Verify split sizes\n",
        "    print(f\"Total chains: {total_chains}\")\n",
        "    print(f\"Training set: {len(train_data)} chains\")\n",
        "    print(f\"Validation set: {len(val_data)} chains\")\n",
        "    print(f\"Test set: {len(test_data)} chains\")\n",
        "\n",
        "    # Save splits\n",
        "    save_dataset(train_data, os.path.join(output_dir, 'train.json'))\n",
        "    save_dataset(val_data, os.path.join(output_dir, 'val.json'))\n",
        "    save_dataset(test_data, os.path.join(output_dir, 'test.json'))"
      ],
      "metadata": {
        "id": "s915Fg4la9sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "input_file = \"synthetic_email_data.json\"  # Path to your dataset\n",
        "output_dir = \"dataset_splits\"  # Directory to save splits\n",
        "train_size = 140  # 70% of 200\n",
        "val_size = 30    # 15% of 200\n",
        "seed = 42        # Random seed for reproducibility"
      ],
      "metadata": {
        "id": "-RjHHNuGbaxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run splitting\n",
        "split_dataset(input_file, output_dir, train_size, val_size, seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk0OqZ40bf3p",
        "outputId": "4b0980a3-6afb-40f0-d2c7-77d6d4a03bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chains: 200\n",
            "Training set: 140 chains\n",
            "Validation set: 30 chains\n",
            "Test set: 30 chains\n",
            "Saved dataset to dataset_splits/train.json\n",
            "Saved dataset to dataset_splits/val.json\n",
            "Saved dataset to dataset_splits/test.json\n"
          ]
        }
      ]
    }
  ]
}